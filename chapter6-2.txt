LAURENCE says: The first step in making our agent multi-modal is to teach it how to interpret the contents of an image. We can do this by using the multi-modal capabilities of Gemma 3.

We'll send the image to Gemma 3 along with a prompt that asks it to describe the image. Gemma will then return a text description of the image.

[Show code from concierge_agent_multimodal.py -- The image interpretation prompt]

As you can see, the prompt is very simple. We're just asking Gemma to "describe this image". But the results are amazing.

In the next video, we'll see how we can integrate this image description into our agentic workflow.
